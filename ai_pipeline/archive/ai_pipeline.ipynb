{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying TabTranformers to OS fingerprinting task using nmap dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf10ddc",
   "metadata": {},
   "source": [
    "### Installing Python dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2024\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../dataset/dataset_no_encoded_4397.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac726051",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Class.vendor_0\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c51b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Class.OSfamily_0\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b46383",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Class.OSgen_0\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_counts = df.groupby(['Class.OSfamily_0', 'Class.OSgen_0']).size().reset_index(name='Count')\n",
    "print(pair_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb099dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Class.device_0\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69eb966",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_counts = df.groupby(['Class.OSfamily_0', 'Class.OSgen_0', \"Class.device_0\"]).size().reset_index(name='Count')\n",
    "print(pair_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa75d21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_counts = df.groupby(['Class.OSfamily_0', \"Class.device_0\"]).size().reset_index(name='Count')\n",
    "print(pair_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.pop('Class.vendor_0')\n",
    "df.pop('Class.OSgen_0')\n",
    "df.pop('Class.device_0')\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# header = names of columns\n",
    "print(list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no of features (X)\n",
    "print(\"NÂº features=\", len(list(df.columns))-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output name\n",
    "OutVar = list(df.columns)[0]\n",
    "print(\"Output=\", OutVar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataCheckings(df):\n",
    "    # Check the number of data points in the data set\n",
    "    print(\"\\nData points =\", len(df))\n",
    "    \n",
    "    # Check the number of columns in the data set\n",
    "    print(\"\\nColumns (output + features)=\",len(df.columns))\n",
    "    \n",
    "    # Check the data types\n",
    "    print(\"\\nData types =\", df.dtypes.unique())\n",
    "    \n",
    "    # Dataset statistics\n",
    "    print('\\n')\n",
    "    df.describe()\n",
    "    \n",
    "    # print names of columns\n",
    "    print('Column Names:\\n', df.columns)\n",
    "    \n",
    "    # see if there are categorical data\n",
    "    print(\"\\nCategorical features:\", df.select_dtypes(include=['O']).columns.tolist())\n",
    "    \n",
    "    # Check NA values\n",
    "    # Check any number of columns with NaN\n",
    "    print(\"\\nColumns with NaN: \", df.isnull().any().sum(), ' / ', len(df.columns))\n",
    "\n",
    "    # Check any number of data points with NaN\n",
    "    print(\"\\nNo of data points with NaN:\", df.isnull().any(axis=1).sum(), ' / ', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataCheckings(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape before removing duplicates=', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates!\n",
    "df.drop_duplicates(keep=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape after removing duplicates=', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove near zero variance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import class_weight\n",
    "# from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getDataFromDataFrame(df, OutVar):\n",
    "#     # get X, Y data and column names from df\n",
    "#     print('\\n-> Get X & Y data, Features list')\n",
    "#     print('Shape', df.shape)\n",
    "    \n",
    "#     # select X and Y\n",
    "#     ds_y = df[OutVar]\n",
    "#     ds_X = df.drop(OutVar,axis = 1)\n",
    "#     Xdata = ds_X.values # get values of features\n",
    "#     Ydata = ds_y.values # get output values\n",
    "\n",
    "#     print('Shape X data:', Xdata.shape)\n",
    "#     print('Shape Y data:', Ydata.shape)\n",
    "    \n",
    "#     # return data for X and Y, feature names as list\n",
    "#     print('Done!')\n",
    "#     return (Xdata, Ydata, list(ds_X.columns))\n",
    "\n",
    "# def Remove0VarCols(df, OutVar):\n",
    "#     Xdata, Ydata, Features = getDataFromDataFrame(df,OutVar=OutVar)# out var = Class \n",
    "#     print('\\n-> Remove zero variance features')\n",
    "#     # print('Initial features:', Features)\n",
    "#     selector= VarianceThreshold()\n",
    "#     Xdata = selector.fit_transform(Xdata)\n",
    "#     # Selected features\n",
    "#     SelFeatures = []\n",
    "#     for i in selector.get_support(indices=True):\n",
    "#         SelFeatures.append(Features[i])\n",
    "#     print('Removed features:',list(set(Features) - set(SelFeatures)))\n",
    "    \n",
    "#     # create the resulted dataframe\n",
    "#     df = pd.DataFrame(Xdata,columns=SelFeatures)\n",
    "#     df[OutVar] = Ydata # add class column\n",
    "#     # print('Final columns:', list(df.columns))\n",
    "#     print('Done!')\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = Remove0VarCols(df, OutVar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print dimension AFTER removing features\n",
    "# print(\"Dataset dimension AFTER removing near zero variance features=\",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the classes ballance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[OutVar].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7943f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(['BSD', 'iOS', 'macOS', 'Solaris', 'Android'], 'Other', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04720b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[OutVar].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d37288",
   "metadata": {},
   "source": [
    "### TabTransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec14670",
   "metadata": {},
   "source": [
    "#### Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d75027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c2ead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import absl.logging\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tab_transformer_pytorch import TabTransformer\n",
    "\n",
    "logging.captureWarnings(True)\n",
    "warnings.filterwarnings('ignore')\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491c8057",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba750d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = OutVar\n",
    "\n",
    "NUMERIC_FEATURES = df.select_dtypes(include=['int64']).columns.tolist()\n",
    "CATEGORICAL_FEATURES = df.select_dtypes(include=['object']).columns.tolist()\n",
    "CATEGORICAL_FEATURES.remove(LABEL)\n",
    "\n",
    "FEATURES = list(NUMERIC_FEATURES) + list(CATEGORICAL_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f5797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(NUMERIC_FEATURES), len(CATEGORICAL_FEATURES), len(FEATURES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7304df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df, stratify=df[LABEL], test_size=0.20, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846f8772",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a571c1c",
   "metadata": {},
   "source": [
    "#### Numeric Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbacc1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='median')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "numeric_pipe = Pipeline([\n",
    "    ('impute', imputer),\n",
    "    ('scale', scaler),\n",
    "])\n",
    "\n",
    "numeric_pipe.fit(train_data[NUMERIC_FEATURES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa142023",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[NUMERIC_FEATURES] = numeric_pipe.transform(train_data[NUMERIC_FEATURES])\n",
    "test_data[NUMERIC_FEATURES] = numeric_pipe.transform(test_data[NUMERIC_FEATURES])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4975692",
   "metadata": {},
   "source": [
    "#### Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b75a4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "categorical_pipe = Pipeline([\n",
    "    ('ordinalencoder', ordinal_encoder),\n",
    "])\n",
    "\n",
    "categorical_pipe.fit(df[CATEGORICAL_FEATURES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748e0147",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[CATEGORICAL_FEATURES] = categorical_pipe.transform(train_data[CATEGORICAL_FEATURES])\n",
    "test_data[CATEGORICAL_FEATURES] = categorical_pipe.transform(test_data[CATEGORICAL_FEATURES])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c06c3f",
   "metadata": {},
   "source": [
    "#### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d0e02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "label_pipe = Pipeline([\n",
    "    ('labelencoder', ordinal_encoder),\n",
    "])\n",
    "\n",
    "label_pipe.fit(df[LABEL].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764ff9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[LABEL] = label_pipe.transform(train_data[LABEL].values.reshape(-1, 1))\n",
    "test_data[LABEL] = label_pipe.transform(test_data[LABEL].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0760a352",
   "metadata": {},
   "source": [
    "#### To Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor_X_cat = torch.tensor(train_data[CATEGORICAL_FEATURES].values).long()\n",
    "train_tensor_X_num = torch.tensor(train_data[NUMERIC_FEATURES].values).float()\n",
    "train_tensor_Y = torch.tensor(train_data[LABEL].values).long()\n",
    "\n",
    "test_tensor_X_cat = torch.tensor(test_data[CATEGORICAL_FEATURES].values).long()\n",
    "test_tensor_X_num = torch.tensor(test_data[NUMERIC_FEATURES].values).float()\n",
    "test_tensor_Y = torch.tensor(test_data[LABEL].values).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4112ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_tensor_X_cat[0:100,:].shape, train_tensor_X_num[0:100,:].shape, train_tensor_Y[0:100].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3e40e7",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(trial):\n",
    "#     embedding_dim = trial.suggest_categorical('embedding_dim',[8, 16, 32, 64])\n",
    "#     depth = trial.suggest_int('depth',1,6,1)\n",
    "#     heads = trial.suggest_int('heads',2,8,1)\n",
    "#     attn_dropout = trial.suggest_float(\"attn_dropout\", 0.05, 0.5)\n",
    "#     ff_dropout = trial.suggest_float(\"ff_dropout\", 0.05, 0.5)\n",
    "#     mlp_hidden_factor1 = trial.suggest_int(\"mlp_hidden_factor1\", 1, 3, 0.5)\n",
    "#     mlp_hidden_factor2 = trial.suggest_int(\"mlp_hidden_factor2\", 1, 3, 0.5)\n",
    "#     use_column_embedding = trial.suggest_categorical('use_column_embedding', [True, False])\n",
    "    \n",
    "#     category_prep_layers = build_categorical_prep(train_data, CATEGORICAL_FEATURES)\n",
    "    \n",
    "#     tabtransformer = TabTransformer(\n",
    "#         numerical_features = NUMERIC_FEATURES,\n",
    "#         categorical_features = CATEGORICAL_FEATURES,\n",
    "#         categorical_lookup=category_prep_layers,\n",
    "#         numerical_discretisers=None, # simply passing the numeric features\n",
    "#         embedding_dim=embedding_dim,\n",
    "#         out_dim=1,\n",
    "#         out_activation='sigmoid',\n",
    "#         depth=depth,\n",
    "#         heads=heads,\n",
    "#         attn_dropout=attn_dropout,\n",
    "#         ff_dropout=ff_dropout,\n",
    "#         mlp_hidden_factors=[mlp_hidden_factor1, mlp_hidden_factor2],\n",
    "#         use_column_embedding=use_column_embedding,\n",
    "#     )\n",
    "    \n",
    "#     LEARNING_RATE = 0.001\n",
    "#     WEIGHT_DECAY = 0.0001\n",
    "#     NUM_EPOCHS = 1000\n",
    "\n",
    "#     optimizer = AdamW(\n",
    "#             learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "#         )\n",
    "\n",
    "#     tabtransformer.compile(\n",
    "#         optimizer = optimizer,\n",
    "#         loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "#         metrics= [tf.keras.metrics.AUC(name=\"AUC\", curve='ROC')],\n",
    "#     )\n",
    "    \n",
    "#     early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20, restore_best_weights=True)\n",
    "#     callback_list = [early]\n",
    "\n",
    "#     history = tabtransformer.fit(\n",
    "#         train_dataset, \n",
    "#         epochs=NUM_EPOCHS, \n",
    "#         validation_data=test_dataset,\n",
    "#         callbacks=callback_list,\n",
    "#         verbose=0\n",
    "#     )\n",
    "    \n",
    "#     val_preds = tabtransformer.predict(test_dataset)\n",
    "#     roc = roc_auc_score(test_dataset[LABEL], val_preds.ravel())\n",
    "    \n",
    "#     gc.collect()\n",
    "    \n",
    "#     return roc\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9555e9a6",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df41f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics import AUROC\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "NUM_EPOCHS = 1000\n",
    "\n",
    "cont_mean_std = torch.zeros(len(train_tensor_X_num.T), 2)\n",
    "for i, column in enumerate(train_tensor_X_num.T):\n",
    "    mean = torch.mean(column)\n",
    "    std = torch.std(column)\n",
    "    cont_mean_std[i] = torch.tensor([mean, std])\n",
    "\n",
    "cat_feature_counts = ()\n",
    "for column in test_tensor_X_cat.T:\n",
    "    unique_values = torch.unique(column)\n",
    "    cat_feature_counts = cat_feature_counts + (len(unique_values),)\n",
    "\n",
    "tabtransformer = TabTransformer(\n",
    "    categories=cat_feature_counts,\n",
    "    num_continuous=len(train_tensor_X_num.T),\n",
    "    dim=32,\n",
    "    dim_out=1,\n",
    "    depth=6,\n",
    "    heads=8,\n",
    "    attn_dropout=0.1,\n",
    "    ff_dropout=0.1,\n",
    "    mlp_hidden_mults=(4, 2),\n",
    "    mlp_act=nn.ReLU(),\n",
    "    continuous_mean_std=cont_mean_std\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(tabtransformer.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "metrics = AUROC('binary')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tabtransformer.to(device)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    tabtransformer.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = tabtransformer(train_tensor_X_cat, train_tensor_X_num)\n",
    "    loss = loss_fn(outputs, train_tensor_Y)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Evaluation\n",
    "    tabtransformer.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = tabtransformer(test_tensor_X_cat, test_tensor_X_num)\n",
    "        val_loss = loss_fn(val_outputs, test_tensor_Y)\n",
    "        val_auc = metrics(val_outputs, test_tensor_Y)\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, Val AUC: {val_auc.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
