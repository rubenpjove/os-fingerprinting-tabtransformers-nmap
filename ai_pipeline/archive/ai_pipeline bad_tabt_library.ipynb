{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying TabTranformers to OS fingerprinting task using nmap dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf10ddc",
   "metadata": {},
   "source": [
    "### Installing Python dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2024\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../dataset/dataset_no_encoded_4397.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_384/527440950.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../dataset/dataset_no_encoded_4397.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../dataset/dataset_no_encoded_4397.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../dataset/dataset_no_encoded_4397.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac726051",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Class.vendor_0\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c51b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Class.OSfamily_0\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b46383",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Class.OSgen_0\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_counts = df.groupby(['Class.OSfamily_0', 'Class.OSgen_0']).size().reset_index(name='Count')\n",
    "print(pair_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb099dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Class.device_0\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69eb966",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_counts = df.groupby(['Class.OSfamily_0', 'Class.OSgen_0', \"Class.device_0\"]).size().reset_index(name='Count')\n",
    "print(pair_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa75d21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_counts = df.groupby(['Class.OSfamily_0', \"Class.device_0\"]).size().reset_index(name='Count')\n",
    "print(pair_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.pop('Class.vendor_0')\n",
    "df.pop('Class.OSgen_0')\n",
    "df.pop('Class.device_0')\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# header = names of columns\n",
    "print(list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no of features (X)\n",
    "print(\"Nº features=\", len(list(df.columns))-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output name\n",
    "OutVar = list(df.columns)[0]\n",
    "print(\"Output=\", OutVar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataCheckings(df):\n",
    "    # Check the number of data points in the data set\n",
    "    print(\"\\nData points =\", len(df))\n",
    "    \n",
    "    # Check the number of columns in the data set\n",
    "    print(\"\\nColumns (output + features)=\",len(df.columns))\n",
    "    \n",
    "    # Check the data types\n",
    "    print(\"\\nData types =\", df.dtypes.unique())\n",
    "    \n",
    "    # Dataset statistics\n",
    "    print('\\n')\n",
    "    df.describe()\n",
    "    \n",
    "    # print names of columns\n",
    "    print('Column Names:\\n', df.columns)\n",
    "    \n",
    "    # see if there are categorical data\n",
    "    print(\"\\nCategorical features:\", df.select_dtypes(include=['O']).columns.tolist())\n",
    "    \n",
    "    # Check NA values\n",
    "    # Check any number of columns with NaN\n",
    "    print(\"\\nColumns with NaN: \", df.isnull().any().sum(), ' / ', len(df.columns))\n",
    "\n",
    "    # Check any number of data points with NaN\n",
    "    print(\"\\nNo of data points with NaN:\", df.isnull().any(axis=1).sum(), ' / ', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataCheckings(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape before removing duplicates=', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates!\n",
    "df.drop_duplicates(keep=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape after removing duplicates=', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove near zero variance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import class_weight\n",
    "# from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getDataFromDataFrame(df, OutVar):\n",
    "#     # get X, Y data and column names from df\n",
    "#     print('\\n-> Get X & Y data, Features list')\n",
    "#     print('Shape', df.shape)\n",
    "    \n",
    "#     # select X and Y\n",
    "#     ds_y = df[OutVar]\n",
    "#     ds_X = df.drop(OutVar,axis = 1)\n",
    "#     Xdata = ds_X.values # get values of features\n",
    "#     Ydata = ds_y.values # get output values\n",
    "\n",
    "#     print('Shape X data:', Xdata.shape)\n",
    "#     print('Shape Y data:', Ydata.shape)\n",
    "    \n",
    "#     # return data for X and Y, feature names as list\n",
    "#     print('Done!')\n",
    "#     return (Xdata, Ydata, list(ds_X.columns))\n",
    "\n",
    "# def Remove0VarCols(df, OutVar):\n",
    "#     Xdata, Ydata, Features = getDataFromDataFrame(df,OutVar=OutVar)# out var = Class \n",
    "#     print('\\n-> Remove zero variance features')\n",
    "#     # print('Initial features:', Features)\n",
    "#     selector= VarianceThreshold()\n",
    "#     Xdata = selector.fit_transform(Xdata)\n",
    "#     # Selected features\n",
    "#     SelFeatures = []\n",
    "#     for i in selector.get_support(indices=True):\n",
    "#         SelFeatures.append(Features[i])\n",
    "#     print('Removed features:',list(set(Features) - set(SelFeatures)))\n",
    "    \n",
    "#     # create the resulted dataframe\n",
    "#     df = pd.DataFrame(Xdata,columns=SelFeatures)\n",
    "#     df[OutVar] = Ydata # add class column\n",
    "#     # print('Final columns:', list(df.columns))\n",
    "#     print('Done!')\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = Remove0VarCols(df, OutVar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print dimension AFTER removing features\n",
    "# print(\"Dataset dimension AFTER removing near zero variance features=\",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the classes ballance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[OutVar].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7943f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(['BSD', 'iOS', 'macOS', 'Solaris', 'Android'], 'Other', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04720b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[OutVar].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d37288",
   "metadata": {},
   "source": [
    "### TabTransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec14670",
   "metadata": {},
   "source": [
    "#### Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d75027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.optimizers import AdamW\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c2ead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import absl.logging\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "from tabtransformertf.models.tabtransformer import TabTransformer\n",
    "from tabtransformertf.utils.preprocessing import df_to_dataset, build_categorical_prep\n",
    "\n",
    "logging.captureWarnings(True)\n",
    "warnings.filterwarnings('ignore')\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491c8057",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7304df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df, stratify=df[OutVar], test_size=0.20, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846f8772",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba750d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = OutVar\n",
    "\n",
    "NUMERIC_FEATURES = df.select_dtypes(include=['int64']).columns.tolist()\n",
    "CATEGORICAL_FEATURES = df.select_dtypes(include=['object']).columns.tolist()\n",
    "CATEGORICAL_FEATURES.remove(LABEL)\n",
    "\n",
    "FEATURES = list(NUMERIC_FEATURES) + list(CATEGORICAL_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f5797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(NUMERIC_FEATURES), len(CATEGORICAL_FEATURES), len(FEATURES))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a571c1c",
   "metadata": {},
   "source": [
    "#### Numeric Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbacc1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='median')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "numeric_pipe = Pipeline([\n",
    "    ('impute', imputer),\n",
    "    ('scale', scaler),\n",
    "])\n",
    "\n",
    "numeric_pipe.fit(train_data[NUMERIC_FEATURES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa142023",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[NUMERIC_FEATURES] = numeric_pipe.transform(train_data[NUMERIC_FEATURES])\n",
    "test_data[NUMERIC_FEATURES] = numeric_pipe.transform(test_data[NUMERIC_FEATURES])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70359da3",
   "metadata": {},
   "source": [
    "#### Dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e47a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[CATEGORICAL_FEATURES] = train_data[CATEGORICAL_FEATURES].astype(str)\n",
    "test_data[CATEGORICAL_FEATURES] = test_data[CATEGORICAL_FEATURES].astype(str)\n",
    "\n",
    "train_data[NUMERIC_FEATURES] = train_data[NUMERIC_FEATURES].astype(int)\n",
    "test_data[NUMERIC_FEATURES] = test_data[NUMERIC_FEATURES].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43652c2",
   "metadata": {},
   "source": [
    "#### Category Lookup Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70400e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_prep_layers = build_categorical_prep(train_data, CATEGORICAL_FEATURES)\n",
    "category_prep_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0760a352",
   "metadata": {},
   "source": [
    "#### To TF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = df_to_dataset(train_data[FEATURES + [LABEL]], LABEL)\n",
    "test_dataset = df_to_dataset(test_data[FEATURES], None, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3e40e7",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    embedding_dim = trial.suggest_categorical('embedding_dim',[8, 16, 32, 64])\n",
    "    depth = trial.suggest_int('depth',1,6,1)\n",
    "    heads = trial.suggest_int('heads',2,8,1)\n",
    "    attn_dropout = trial.suggest_float(\"attn_dropout\", 0.05, 0.5)\n",
    "    ff_dropout = trial.suggest_float(\"ff_dropout\", 0.05, 0.5)\n",
    "    mlp_hidden_factor1 = trial.suggest_int(\"mlp_hidden_factor1\", 1, 3, 0.5)\n",
    "    mlp_hidden_factor2 = trial.suggest_int(\"mlp_hidden_factor2\", 1, 3, 0.5)\n",
    "    use_column_embedding = trial.suggest_categorical('use_column_embedding', [True, False])\n",
    "    \n",
    "    category_prep_layers = build_categorical_prep(train_data, CATEGORICAL_FEATURES)\n",
    "    \n",
    "    tabtransformer = TabTransformer(\n",
    "        numerical_features = NUMERIC_FEATURES,\n",
    "        categorical_features = CATEGORICAL_FEATURES,\n",
    "        categorical_lookup=category_prep_layers,\n",
    "        numerical_discretisers=None, # simply passing the numeric features\n",
    "        embedding_dim=embedding_dim,\n",
    "        out_dim=1,\n",
    "        out_activation='sigmoid',\n",
    "        depth=depth,\n",
    "        heads=heads,\n",
    "        attn_dropout=attn_dropout,\n",
    "        ff_dropout=ff_dropout,\n",
    "        mlp_hidden_factors=[mlp_hidden_factor1, mlp_hidden_factor2],\n",
    "        use_column_embedding=use_column_embedding,\n",
    "    )\n",
    "    \n",
    "    LEARNING_RATE = 0.001\n",
    "    WEIGHT_DECAY = 0.0001\n",
    "    NUM_EPOCHS = 1000\n",
    "\n",
    "    optimizer = AdamW(\n",
    "            learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "        )\n",
    "\n",
    "    tabtransformer.compile(\n",
    "        optimizer = optimizer,\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics= [tf.keras.metrics.AUC(name=\"AUC\", curve='ROC')],\n",
    "    )\n",
    "    \n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20, restore_best_weights=True)\n",
    "    callback_list = [early]\n",
    "\n",
    "    history = tabtransformer.fit(\n",
    "        train_dataset, \n",
    "        epochs=NUM_EPOCHS, \n",
    "        validation_data=test_dataset,\n",
    "        callbacks=callback_list,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    val_preds = tabtransformer.predict(test_dataset)\n",
    "    roc = roc_auc_score(test_dataset[LABEL], val_preds.ravel())\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return roc\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9555e9a6",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df41f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabtransformer = TabTransformer(\n",
    "        numerical_features = NUMERIC_FEATURES,\n",
    "        categorical_features = CATEGORICAL_FEATURES,\n",
    "        categorical_lookup=category_prep_layers,\n",
    "        numerical_discretisers=None, # simply passing the numeric features\n",
    "        embedding_dim=32,\n",
    "        out_dim=1,\n",
    "        out_activation='sigmoid',\n",
    "        depth=6,\n",
    "        heads=5,\n",
    "        attn_dropout=0.087687,\n",
    "        ff_dropout=0.429539,\n",
    "        mlp_hidden_factors=[1, 1],\n",
    "        use_column_embedding=False,\n",
    "    )\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "NUM_EPOCHS = 1000\n",
    "\n",
    "optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "\n",
    "tabtransformer.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics= [tf.keras.metrics.AUC(name=\"AUC\", curve='ROC')],\n",
    ")\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20, restore_best_weights=True)\n",
    "callback_list = [early]\n",
    "\n",
    "history = tabtransformer.fit(\n",
    "    train_dataset, \n",
    "    epochs=NUM_EPOCHS, \n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callback_list,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classic ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, roc_auc_score,f1_score, recall_score, precision_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_weights(y_data, option='balanced'):\n",
    "    \"\"\"Estimate class weights for umbalanced dataset\n",
    "       If ‘balanced’, class weights will be given by n_samples / (n_classes * np.bincount(y)). \n",
    "       If a dictionary is given, keys are classes and values are corresponding class weights. \n",
    "       If None is given, the class weights will be uniform \"\"\"\n",
    "    cw = class_weight.compute_class_weight(option, np.unique(y_data), y_data)\n",
    "    w = {i:j for i,j in zip(np.unique(y_data), cw)}\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = set_weights(Ydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classes=\",class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a list of classifiers to train as baseline classifiers\n",
    "classifiers = [\n",
    "    GaussianNB(),\n",
    "    LinearDiscriminantAnalysis(), # No random_state\n",
    "    LogisticRegression(n_jobs=-1,solver='lbfgs',random_state=seed,class_weight=class_weights),\n",
    "    MLPClassifier(hidden_layer_sizes= (30), random_state = seed, shuffle=False, solver='adam',activation='relu',batch_size=500, max_iter=5000),\n",
    "    DecisionTreeClassifier(random_state=seed,class_weight=class_weights),\n",
    "    RandomForestClassifier(n_jobs=-1,random_state=seed,class_weight=class_weights),\n",
    "    BaggingClassifier(n_jobs=-1,random_state=seed),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and metrics (ACC, precision, recall, f1score) for a classifier\n",
    "def ML_baseline(cls, X_tr, y_tr, X_ts, y_ts, seed=42, classes=['0','1']):\n",
    "    ACC = 0\n",
    "    AUROC = 0\n",
    "    precision = 0 \n",
    "    recall = 0\n",
    "    f1score = 0\n",
    "    \n",
    "    cls_name = type(cls).__name__\n",
    "    \n",
    "    start_time = time.time()\n",
    "    cls.fit(X_tr, y_tr) # TRAINING!\n",
    "    print('\\n---->', \"training: %0.2f mins \\n\\n\" % ((time.time() - start_time)/60))\n",
    "    \n",
    "    # predictions\n",
    "    y_pred  = cls.predict(X_ts)             # predict classes\n",
    "    y_probs = cls.predict_proba(X_ts)[:, 1] # predict probabilities of classes\n",
    "    cls_rep = classification_report(y_ts, y_pred, target_names=classes,\n",
    "                                    output_dict=True, digits=3)\n",
    "    # print classification report\n",
    "    #print(cls_rep)\n",
    "    \n",
    "    ACC       = accuracy_score(y_ts, y_pred)\n",
    "    #AUROC     = roc_auc_score(y_ts, y_probs) # this is working for 2-classes classification only!!!\n",
    "    precision = cls_rep['weighted avg']['precision']\n",
    "    recall    = cls_rep['weighted avg']['recall']\n",
    "    f1score   = cls_rep['weighted avg']['f1-score']  \n",
    "    \n",
    "    # print metrics\n",
    "    print(\"\\n\", \"ACC=\", ACC, \"precision=\", precision, \"recall=\", recall, \"f1score=\",f1score)\n",
    "    \n",
    "    return cls, ACC, precision, recall, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for ML baseline\n",
    "df_ML = pd.DataFrame(columns=['Method', 'ACC','precision' ,'recall' ,'f1-score' ])\n",
    "\n",
    "for cls in classifiers:\n",
    "    print(\"\\n**********************************\\n\", cls_name = type(cls).__name__)\n",
    "    cls_fit, ACC, precision,recall,f1score=ML_baseline(cls, X_train, y_train, X_test, y_test, seed=seed,classes=['1','2','3','4','5','6','7'])\n",
    "    df_ML = df_ML.append({'Method': str(type(cls).__name__),\n",
    "                          'ACC': float(ACC),\n",
    "                          #'AUROC': float(AUROC),\n",
    "                          'precision': float(precision),\n",
    "                          'recall': float(recall),\n",
    "                          'f1-score': float(f1score)}, ignore_index=True)\n",
    "\n",
    "df_ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ML.to_csv('ML_results.csv', index=False) # write to file the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a better classifier for the best ML method\n",
    "\n",
    "We are using the best methods from baseline to find better hyperparameters for a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out best model was RF:\n",
    "cls=RandomForestClassifier(n_jobs=-1,random_state=seed,class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all the parameters\n",
    "cls.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a list of classifiers to train with different params\n",
    "classifiers = [\n",
    "    RandomForestClassifier(n_estimators=10, n_jobs=-1,random_state=seed,class_weight=class_weights),\n",
    "    RandomForestClassifier(n_estimators=20, n_jobs=-1,random_state=seed,class_weight=class_weights),\n",
    "    RandomForestClassifier(n_estimators=50, n_jobs=-1,random_state=seed,class_weight=class_weights),\n",
    "    RandomForestClassifier(n_estimators=100, n_jobs=-1,random_state=seed,class_weight=class_weights),\n",
    "    RandomForestClassifier(n_estimators=200, n_jobs=-1,random_state=seed,class_weight=class_weights),\n",
    "    RandomForestClassifier(n_estimators=300, n_jobs=-1,random_state=seed,class_weight=class_weights),\n",
    "    RandomForestClassifier(n_estimators=500, n_jobs=-1,random_state=seed,class_weight=class_weights),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for ML baseline\n",
    "df_ML2 = pd.DataFrame(columns=['Method', 'ACC','precision' ,'recall' ,'f1-score' ])\n",
    "df_ML2\n",
    "\n",
    "for cls in classifiers:\n",
    "    print(\"\\n**********************************\\n\", cls)\n",
    "    cls_fit, ACC, precision,recall,f1score=ML_baseline(cls, X_train, y_train, X_test, y_test, seed=seed,classes=['Android', 'BSD', 'Linux', 'Solaris', 'Windows', 'iOS', 'macOS'])\n",
    "    df_ML2 = df_ML2.append({'Method': str(type(cls).__name__)+'-NoTrees='+str(cls.get_params()['n_estimators']),\n",
    "                            'ACC': float(ACC),\n",
    "                            #'AUROC': float(AUROC),\n",
    "                            'precision': float(precision),\n",
    "                            'recall': float(recall),\n",
    "                            'f1-score': float(f1score)}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ML2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ML2.to_csv('ML_results_best1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(cls.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search - search for the best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramsx = {'bootstrap': [True, False],\n",
    "           'max_depth': [10, 20, 30, 40, 50, None],\n",
    "           'max_features': ['auto', 'sqrt'],\n",
    "           'min_samples_leaf': [1, 2, 4],\n",
    "           'min_samples_split': [2, 5, 10],\n",
    "           'n_estimators': [50]\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest= RandomForestClassifier(random_state=seed,class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridF = GridSearchCV(forest, paramsx, cv = 3, verbose = 2, n_jobs = -1)\n",
    "bestF = gridF.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestF.best_params_ # params of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    y_pred = model.predict(test_features)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    print (accuracy)\n",
    "    print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid = bestF.best_estimator_ # the best model from grid search\n",
    "\n",
    "evaluate(best_grid,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate ACC\n",
    "y_pred=clf.predict(X_test)\n",
    "print(list(clf.classes_))\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "feature_imp = pd.Series(clf.feature_importances_,index=df.columns[:-1]).sort_values(ascending=False)\n",
    "feature_imp[:30]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
